{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "100knock_9-3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNjMFHwhjHPR6Yg4OPWO9Pp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaruSora/NLP_100knock/blob/main/100knock_9_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch==1.6.0\n",
        "! pip install torchtext==0.7.0\n",
        "! pip install torch_optimizer\n",
        "! pip install pytorch-lightning==1.0.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NurYPlSPhYKk",
        "outputId": "4e98ea20-c5f0-4393-a79c-9884005da1fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 12 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.21.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.7.0\n",
            "  Downloading torchtext-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7.0) (1.21.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7.0) (4.63.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7.0) (1.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7.0) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7.0) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.7.0) (0.16.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "Successfully installed sentencepiece-0.1.96 torchtext-0.7.0\n",
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 358 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (1.6.0)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->torch_optimizer) (1.21.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->torch_optimizer) (0.16.0)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.3.0\n",
            "Collecting pytorch-lightning==1.0.8\n",
            "  Downloading pytorch_lightning-1.0.8-py3-none-any.whl (561 kB)\n",
            "\u001b[K     |████████████████████████████████| 561 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 49.9 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 53.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.0.8) (2.8.0)\n",
            "Collecting fsspec>=0.8.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 65.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.0.8) (4.63.0)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.0.8) (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.0.8) (1.21.5)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (1.44.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (3.2.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=1afe17e754431153321f835200bc7d6e9e08a87d218c91cdd9b1c5b5bd6a4a45\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: future, PyYAML, fsspec, pytorch-lightning\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 fsspec-2022.3.0 future-0.18.2 pytorch-lightning-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
        "!unzip NewsAggregatorDataset.zip\n",
        "!sed -e 's/\"/'\\''/g' newsCorpora.csv > newsCorpora_re.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUAaJgPsgiCN",
        "outputId": "9984fc2c-0208-4b7a-ff78-bba0fdb1caaa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-01 05:04:07--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29224203 (28M) [application/x-httpd-php]\n",
            "Saving to: ‘NewsAggregatorDataset.zip’\n",
            "\n",
            "NewsAggregatorDatas 100%[===================>]  27.87M  13.2MB/s    in 2.1s    \n",
            "\n",
            "2022-04-01 05:04:10 (13.2 MB/s) - ‘NewsAggregatorDataset.zip’ saved [29224203/29224203]\n",
            "\n",
            "Archive:  NewsAggregatorDataset.zip\n",
            "  inflating: 2pageSessions.csv       \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._2pageSessions.csv  \n",
            "  inflating: newsCorpora.csv         \n",
            "  inflating: __MACOSX/._newsCorpora.csv  \n",
            "  inflating: readme.txt              \n",
            "  inflating: __MACOSX/._readme.txt   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 50のコードをそのまま利用\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
        "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
        "\n",
        "train, tmp = train_test_split(df, test_size=0.2, shuffle=True, random_state=42, stratify=df['CATEGORY'])\n",
        "valid, test = train_test_split(tmp, test_size=0.5, shuffle=True, random_state=42, stratify=tmp['CATEGORY'])\n",
        "\n",
        "train.to_csv('train.txt', sep='\\t', index=False)\n",
        "valid.to_csv('valid.txt', sep='\\t', index=False)\n",
        "test.to_csv('test.txt', sep='\\t', index=False)\n",
        "\n",
        "print('学習用データ')\n",
        "print(train['CATEGORY'].value_counts())\n",
        "print('評価用データ')\n",
        "print(valid['CATEGORY'].value_counts())\n",
        "print('テスト用データ')\n",
        "print(test['CATEGORY'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p0xwkktgnIV",
        "outputId": "b8875865-b029-4d48-ef96-c5609bb6f9ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "学習用データ\n",
            "b    4501\n",
            "e    4235\n",
            "t    1220\n",
            "m     728\n",
            "Name: CATEGORY, dtype: int64\n",
            "評価用データ\n",
            "b    563\n",
            "e    529\n",
            "t    153\n",
            "m     91\n",
            "Name: CATEGORY, dtype: int64\n",
            "テスト用データ\n",
            "b    563\n",
            "e    530\n",
            "t    152\n",
            "m     91\n",
            "Name: CATEGORY, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "import torch_optimizer as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import Example, Field, Dataset, BucketIterator\n",
        "from torchtext.vocab import FastText\n",
        "import string\n",
        "import os\n",
        "\n",
        "# データ読み込み\n",
        "text_field = Field(sequential=True, use_vocab=True)\n",
        "label_field = Field(sequential=False, use_vocab=False, is_target=True)\n",
        "fields = [(\"x\", text_field), (\"t\", label_field)]\n",
        "\n",
        "table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "label2id = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
        "batch_size = 1\n",
        "\n",
        "def load_corpus(fname):\n",
        "    examples = list()\n",
        "    with open(fname, 'r') as f:\n",
        "        sentences = df[\"TITLE\"]\n",
        "        labels = df[\"CATEGORY\"]\n",
        "        for sentence, label in zip(sentences, labels):\n",
        "            word_list = sentence.translate(table).split()\n",
        "            label_id = label2id[label]\n",
        "            examples.append(Example.fromlist([word_list, label_id], fields))\n",
        "        return Dataset(examples, fields)\n",
        "\n",
        "train_dataset = load_corpus('train.txt')\n",
        "valid_dataset = load_corpus('valid.txt')\n",
        "test_dataset = load_corpus('test.txt')\n",
        "\n",
        "text_field.build_vocab(train_dataset, vectors=FastText(language=\"en\"), min_freq=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HkoqDOrgoLJ",
        "outputId": "2bccdca8-7a45-426c-dc9e-049d8cb34d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
            ".vector_cache/wiki.en.vec:  18%|█▊        | 1.17G/6.60G [3:02:30<15:26:06, 97.7kB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyRNN(pl.LightningModule):\n",
        "    # 層の定義\n",
        "    def __init__(self, n_input, n_emb, n_hidden, n_layers, n_output, dropout, bidirectional, lr):\n",
        "        super(MyRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(num_embeddings=n_input, embedding_dim=n_emb, padding_idx=1)\n",
        "        self.embed.weight.data.copy_(text_field.vocab.vectors)\n",
        "        self.lstm = nn.LSTM(input_size=n_emb, hidden_size=n_hidden, num_layers=n_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        self.fc = nn.Linear(in_features=n_hidden * (2 if bidirectional==True else 1), out_features=n_output)    \n",
        "    # 順伝播\n",
        "    def forward(self, x):\n",
        "        o, (h, c) = self.lstm(self.embed(x))\n",
        "        return self.fc(o[-1])\n",
        "    # 訓練用データの損失計算\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, t = batch\n",
        "        y = self(x)\n",
        "        loss = self.lossfun(y, t)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "    # 検証用データの損失計算\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, t = batch\n",
        "        y = self(x)\n",
        "        loss = self.lossfun(y, t)\n",
        "        self.log(\"val_loss\", loss)\n",
        "    # 評価用データの正解率計算\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, t = batch\n",
        "        y = self(x) \n",
        "        y = torch.argmax(y, dim=1)\n",
        "        accuracy = torch.sum(t == y).item() / (len(y) * 1.0)\n",
        "        self.log(\"test_acc\", accuracy)\n",
        "    # 損失関数の定義\n",
        "    def lossfun(self, y, t):\n",
        "        return F.cross_entropy(y, t)\n",
        "    # オプティマイザの定義\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(self.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "a_IdR_qCgvtR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "88.問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"
      ],
      "metadata": {
        "id": "T_t8zmtmftUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "LXz8TPoPgzow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 88 パラメータチューニング\n",
        "\n",
        "import optuna\n",
        "\n",
        "# optunaによるハイパーパラメータチューニング\n",
        "def objective(trial):\n",
        "    n_hidden = int(trial.suggest_discrete_uniform('n_hidden', 100, 200, 50))\n",
        "    n_layers = int(trial.suggest_discrete_uniform('n_layers', 1, 3, 1))\n",
        "    batch_size = int(trial.suggest_discrete_uniform('batch_size', 32, 64, 32))\n",
        "\n",
        "    n_input = len(text_field.vocab)\n",
        "    n_embed = 300\n",
        "    n_output = len(label2id)\n",
        "    bidirectional = True\n",
        "    dropout = 0.3\n",
        "    lr = 0.01\n",
        "\n",
        "    train_dataloader = BucketIterator(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    valid_dataloader = BucketIterator(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_dataloader = BucketIterator(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "    model = MyRNN(n_input, n_embed, n_hidden, n_layers, n_output, dropout, bidirectional, lr)\n",
        "\n",
        "    # 訓練中にモデルを保存するための設定\n",
        "    checkpoint = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1,save_weights_only=True, dirpath=\"model/\")\n",
        " \n",
        "    # 訓練\n",
        "    trainer = pl.Trainer(gpus=1, max_epochs=20, callbacks=[checkpoint])\n",
        "    trainer.fit(model, train_dataloader, valid_dataloader)\n",
        "\n",
        "    valid_loss = checkpoint.best_model_score\n",
        "\n",
        "    return valid_loss"
      ],
      "metadata": {
        "id": "W1nMIJSafrRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "print('  Value: {:.3f}'.format(trial.value))\n",
        "print('  Params: ')\n",
        "for key, value in trial.params.items():\n",
        "  print('    {}: {}'.format(key, value))"
      ],
      "metadata": {
        "id": "RTFJ2b57vECY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = trial.params['batch_size']\n",
        "\n",
        "train_dataloader = BucketIterator(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = BucketIterator(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = BucketIterator(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "n_input = len(text_field.vocab)\n",
        "n_embed = 300\n",
        "n_hidden = int(trial.params['n_hidden'])\n",
        "n_layers = int(trial.params['n_layers'])\n",
        "n_output = len(label2id)\n",
        "dropout = 0.3\n",
        "lr = 0.01\n",
        "bidirectional = True\n",
        "\n",
        "model = MyRNN(n_input, n_embed, n_hidden, n_layers, n_output, dropout, bidirectional, lr)\n",
        "\n",
        "# 訓練中にモデルを保存するための設定\n",
        "checkpoint = pl.callbacks.ModelCheckpoint(\n",
        "    # 検証用データにおける損失が最も小さいモデルを保存する\n",
        "    monitor=\"val_loss\", mode=\"min\", save_top_k=1,\n",
        "    # モデルファイル（重みのみ）を \"model\" というディレクトリに保存する\n",
        "    save_weights_only=True, dirpath=\"model/\"\n",
        ")\n",
        "\n",
        "# 訓練\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=10, callbacks=[checkpoint])\n",
        "trainer.fit(model, train_dataloader, valid_dataloader)\n",
        "\n",
        "# ベストモデルの確認\n",
        "print(\"ベストモデル: \", checkpoint.best_model_path)\n",
        "print(\"ベストモデルの検証用データにおける損失: \", checkpoint.best_model_score)\n",
        "\n",
        "# 評価\n",
        "test = trainer.test(test_dataloaders=test_dataloader)\n",
        "print(\"Test accuracy = %.3f\" % (test[0][\"test_acc\"]))"
      ],
      "metadata": {
        "id": "wEqqNsx7wnxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "89.事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"
      ],
      "metadata": {
        "id": "9wn7D7iRfr4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def calc_loss_acc(model, device, dataloader, criterion):\n",
        "    loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            ids = data['ids'].to(device)\n",
        "            mask = data['mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "            y = model(ids, mask)\n",
        "\n",
        "            if criterion != None:\n",
        "                loss += criterion(y, labels).item()\n",
        "\n",
        "            pred = torch.argmax(y, dim=-1).cpu().numpy()\n",
        "            labels = torch.argmax(labels, dim=-1).cpu().numpy()\n",
        "            total += len(labels)\n",
        "            correct += (pred == labels).sum().item()\n",
        "\n",
        "    return loss / len(dataloader), correct / total\n",
        "\n",
        "def model_function(train_dataset, valid_dataset, batch_size, device, model, criterion, optimizer, max_epoch):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
        "\n",
        "    train_log = []\n",
        "    valid_log = []\n",
        "    \n",
        "    for epoch in range(max_epoch):\n",
        "        start = time.time()\n",
        "\n",
        "        model.train()\n",
        "        for data in train_loader:\n",
        "            ids = data['ids'].to(device)\n",
        "            mask = data['mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y = model(ids, mask)\n",
        "            loss = criterion(y, labels)        \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        train_loss, train_acc = calc_loss_acc(model, device, train_loader, criterion)\n",
        "        valid_loss, valid_acc = calc_loss_acc(model, device, valid_loader, criterion)\n",
        "        train_log.append([train_loss, train_acc])\n",
        "        valid_log.append([valid_loss, valid_acc])   \n",
        "\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "        end = time.time() \n",
        "\n",
        "        print(f'epoch: {epoch + 1}, loss_train: {train_loss:.4f}, accuracy_train: {train_acc:.4f}, loss_valid: {valid_loss:.4f}, accuracy_valid: {valid_acc:.4f}, {(end - start):.4f}sec') \n",
        "\n",
        "    return {'train': train_log, 'valid': valid_log}"
      ],
      "metadata": {
        "id": "yrXuaYXtGV0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BertClass(torch.nn.Module):\n",
        "    def __init__(self, dropout, output_size):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.fc = torch.nn.Linear(768, output_size)\n",
        "\n",
        "    def forward(self, ids, mask):\n",
        "        _, output = self.bert(ids, attention_mask=mask, return_dict=False)\n",
        "        output = self.fc(self.dropout(output))\n",
        "        return output    \n",
        "\n",
        "dropout = 0.3\n",
        "output_size = 4\n",
        "batch_size = 16\n",
        "max_epochs = 5\n",
        "lr = 2e-5\n",
        "\n",
        "model = BertClass(dropout, output_size)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "model_function(train_dataset, valid_dataset, batch_size, device, model, criterion, optimizer, max_epochs)"
      ],
      "metadata": {
        "id": "TAQNLYPQf6k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
        "_, train_acc = loss_and_accuracy(model, device, train_loader, criterion)\n",
        "_, valid_acc = loss_and_accuracy(model, device, valid_loader, criterion)\n",
        "_, test_acc = loss_and_accuracy(model, device, test_loader, criterion)\n",
        "print(f'正解率（学習データ）：{train_acc:.3f}')\n",
        "print(f'正解率（学習データ）：{valid_acc:.3f}')\n",
        "print(f'正解率（評価データ）：{test_acc:.3f}')"
      ],
      "metadata": {
        "id": "u85YJc8rG45Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}