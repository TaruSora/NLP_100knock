{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "100knock_10.ipynb",
      "provenance": [],
      "mount_file_id": "10_FV-ZQGpLgUiI27Le-tPfOwzdmep4pB",
      "authorship_tag": "ABX9TyPL2/vwgvSTNz1FAWnrPeVb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaruSora/NLP_100knock/blob/main/100knock_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "90.機械翻訳のデータセットをダウンロードせよ．訓練データ，開発データ，評価データを整形し，必要に応じてトークン化などの前処理を行うこと．ただし，この段階ではトークンの単位として形態素（日本語）および単語（英語）を採用せよ."
      ],
      "metadata": {
        "id": "WjU2evSByN6w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqD7RIXQx97i",
        "outputId": "08e18fbf-2b8b-40b0-e2c3-8c7cfbb4dff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-01 06:28:39--  http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n",
            "Resolving www.phontron.com (www.phontron.com)... 208.113.196.149\n",
            "Connecting to www.phontron.com (www.phontron.com)|208.113.196.149|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99246893 (95M) [application/gzip]\n",
            "Saving to: ‘kftt-data-1.0.tar.gz’\n",
            "\n",
            "kftt-data-1.0.tar.g 100%[===================>]  94.65M  99.3MB/s    in 1.0s    \n",
            "\n",
            "2022-04-01 06:28:42 (99.3 MB/s) - ‘kftt-data-1.0.tar.gz’ saved [99246893/99246893]\n",
            "\n",
            "kftt-data-1.0/\n",
            "kftt-data-1.0/data/\n",
            "kftt-data-1.0/data/orig/\n",
            "kftt-data-1.0/data/orig/kyoto-tune.en\n",
            "kftt-data-1.0/data/orig/kyoto-dev.ja\n",
            "kftt-data-1.0/data/orig/kyoto-dev.en\n",
            "kftt-data-1.0/data/orig/kyoto-train.en\n",
            "kftt-data-1.0/data/orig/kyoto-tune.ja\n",
            "kftt-data-1.0/data/orig/kyoto-train.ja\n",
            "kftt-data-1.0/data/orig/kyoto-test.ja\n",
            "kftt-data-1.0/data/orig/kyoto-test.en\n",
            "kftt-data-1.0/data/tok/\n",
            "kftt-data-1.0/data/tok/kyoto-tune.en\n",
            "kftt-data-1.0/data/tok/kyoto-dev.ja\n",
            "kftt-data-1.0/data/tok/kyoto-train.cln.en\n",
            "kftt-data-1.0/data/tok/kyoto-dev.en\n",
            "kftt-data-1.0/data/tok/kyoto-train.en\n",
            "kftt-data-1.0/data/tok/kyoto-tune.ja\n",
            "kftt-data-1.0/data/tok/kyoto-train.cln.ja\n",
            "kftt-data-1.0/data/tok/kyoto-train.ja\n",
            "kftt-data-1.0/data/tok/kyoto-test.ja\n",
            "kftt-data-1.0/data/tok/kyoto-test.en\n",
            "kftt-data-1.0/README.txt\n",
            "\u001b[K     |████████████████████████████████| 59.1 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 52.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 48.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 451 kB 50.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 61.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 36.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 653 kB 50.7 MB/s \n",
            "\u001b[?25h  Building wheel for sudachidict-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 16.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.63.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# 90 データの準備\n",
        "\n",
        "! wget http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n",
        "! tar xvzf kftt-data-1.0.tar.gz\n",
        "\n",
        "! pip install -U --quiet ja_ginza\n",
        "! python -m spacy download en_core_web_sm\n",
        "\n",
        "! mkdir -p /content/ginza-data\n",
        "! cat kftt-data-1.0/data/orig/kyoto-train.ja | sed 's/\\s+/ /g' | ginzame > /content/ginza-data/train.ginza.ja\n",
        "! cat kftt-data-1.0/data/orig/kyoto-dev.ja | sed 's/\\s+/ /g' | ginzame > /content/ginza-data/dev.ginza.ja\n",
        "! cat kftt-data-1.0/data/orig/kyoto-test.ja | sed 's/\\s+/ /g' | ginzame > /content/ginza-data/test.ginza.ja"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data\n",
        "for src, dst in [\n",
        "    (\"/content/ginza-data/train.ginza.ja\", \"/content/data/train.ja\"),\n",
        "    (\"/content/ginza-data/dev.ginza.ja\", \"/content/data/dev.ja\"),\n",
        "    (\"/content/ginza-data/test.ginza.ja\", \"/content/data/test.ja\"),\n",
        "]:\n",
        "    with open(src) as f:\n",
        "        list = []\n",
        "        tmp = []\n",
        "        for x in f:\n",
        "            x = x.strip()\n",
        "            if x == 'EOS':\n",
        "                list.append(' '.join(tmp))\n",
        "                tmp = []\n",
        "            elif x != '':\n",
        "                tmp.append(x.split('\\t')[0])\n",
        "    with open(dst, 'w') as f:\n",
        "        for line in list:\n",
        "            print(line, file=f)"
      ],
      "metadata": {
        "id": "eddvWkHE0zDC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "for src, dst in [\n",
        "    (\"kftt-data-1.0/data/orig/kyoto-train.en\", \"/content/data/train.en\"),\n",
        "    (\"kftt-data-1.0/data/orig/kyoto-dev.en\", \"/content/data/dev.en\"),\n",
        "    (\"kftt-data-1.0/data/orig/kyoto-test.en\", \"/content/data/test.en\"),\n",
        "]:\n",
        "    with open(src) as f, open(dst, 'w') as g:\n",
        "        for x in f:\n",
        "            x = x.strip()\n",
        "            x = re.sub(r'\\s+', ' ', x)\n",
        "            x = nlp.make_doc(x)\n",
        "            x = ' '.join([doc.text for doc in x])\n",
        "            print(x, file=g)"
      ],
      "metadata": {
        "id": "Xn7sl8XT1_Sw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "91.90で準備したデータを用いて，ニューラル機械翻訳のモデルを学習せよ（ニューラルネットワークのモデルはTransformerやLSTMなど適当に選んでよい）．"
      ],
      "metadata": {
        "id": "4dtK1oqXyRsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 91 機械翻訳モデルの訓練\n",
        "\n",
        "!git clone https://github.com/pytorch/fairseq\n",
        "%cd /content/fairseq/\n",
        "!python -m pip install --editable .\n",
        "%cd /content\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "\n",
        "!fairseq-preprocess -s ja -t en \\\n",
        "    --trainpref /content/data/train \\\n",
        "    --validpref /content/data/dev \\\n",
        "    --destdir /content/data \\\n",
        "    --thresholdsrc 5 \\\n",
        "    --thresholdtgt 5 \\\n",
        "    --workers 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv0bCo16yRyh",
        "outputId": "50398fef-834c-4806-e396-5324cd0e5805"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 31117, done.\u001b[K\n",
            "remote: Counting objects: 100% (748/748), done.\u001b[K\n",
            "remote: Compressing objects: 100% (401/401), done.\u001b[K\n",
            "remote: Total 31117 (delta 371), reused 618 (delta 335), pack-reused 30369\u001b[K\n",
            "Receiving objects: 100% (31117/31117), 21.33 MiB | 27.03 MiB/s, done.\n",
            "Resolving deltas: 100% (23070/23070), done.\n",
            "/content/fairseq\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 27.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+06c65c8) (2019.12.20)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+06c65c8) (0.29.28)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+06c65c8) (1.21.5)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+06c65c8) (4.63.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+06c65c8) (1.10.0+cu111)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+06c65c8) (0.10.0+cu111)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[K     |████████████████████████████████| 223 kB 69.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+06c65c8) (1.15.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+06c65c8) (5.4.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 55.6 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1.*\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 64.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+06c65c8) (3.10.0.2)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+06c65c8) (0.8.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+06c65c8) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+06c65c8) (3.7.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=06e428efb7be983d395eeaa1c8de30448ae8ec56c7d8d8c59a9c219f2ea48775\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: PyYAML, portalocker, omegaconf, colorama, antlr4-python3-runtime, sacrebleu, hydra-core, bitarray, fairseq\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.8 bitarray-2.4.1 colorama-0.4.4 fairseq hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.4.0 sacrebleu-2.0.0\n",
            "/content\n",
            "2022-04-01 06:41:53 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-04-01 06:41:54 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/content/data', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='ja', srcdict=None, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=5, thresholdtgt=5, tokenizer=None, tpu=False, trainpref='/content/data/train', use_plasma_view=False, user_dir=None, validpref='/content/data/dev', wandb_project=None, workers=20)\n",
            "2022-04-01 06:43:31 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 53928 types\n",
            "2022-04-01 06:45:18 | INFO | fairseq_cli.preprocess | [ja] /content/data/train.ja: 440288 sents, 11550262 tokens, 1.24% replaced (by <unk>)\n",
            "2022-04-01 06:45:18 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 53928 types\n",
            "2022-04-01 06:45:20 | INFO | fairseq_cli.preprocess | [ja] /content/data/dev.ja: 1166 sents, 26468 tokens, 1.24% replaced (by <unk>)\n",
            "2022-04-01 06:45:20 | INFO | fairseq_cli.preprocess | [en] Dictionary: 55472 types\n",
            "2022-04-01 06:46:40 | INFO | fairseq_cli.preprocess | [en] /content/data/train.en: 440288 sents, 12321279 tokens, 1.56% replaced (by <unk>)\n",
            "2022-04-01 06:46:40 | INFO | fairseq_cli.preprocess | [en] Dictionary: 55472 types\n",
            "2022-04-01 06:46:41 | INFO | fairseq_cli.preprocess | [en] /content/data/dev.en: 1166 sents, 26101 tokens, 2.83% replaced (by <unk>)\n",
            "2022-04-01 06:46:41 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-train /content/data --arch transformer \\\n",
        "    --criterion label_smoothed_cross_entropy \\\n",
        "    --label-smoothing 0.1 \\\n",
        "    --lr 1e-5 \\\n",
        "    --lr-scheduler inverse_sqrt \\\n",
        "    --warmup-updates 3000 \\\n",
        "    --optimizer adam \\\n",
        "    --max-tokens 4000 \\\n",
        "    --max-epoch 3 \\\n",
        "    --dropout 0.1 \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --weight-decay 0.0001 \\\n",
        "    --patience 5 \\\n",
        "    --no-epoch-checkpoints > 91.log \\\n",
        "\n",
        "!cp /content/checkpoints/checkpoint_best.pt \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best.pt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvARpcPi2fw3",
        "outputId": "10998cc8-94df-430c-aece-465eb2cbde96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\repoch 001:   0% 0/3618 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "epoch 001:   8% 287/3618 [2:16:56<24:34:47, 26.56s/it, loss=15.368, nll_loss=15.245, ppl=38834.4, wps=120.2, ups=0.03, wpb=3459.6, bsz=118.2, num_updates=200, lr=6.66667e-07, gnorm=7.958, train_wall=2872, wall=5788]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "92.91で学習したニューラル機械翻訳モデルを用い，与えられた（任意の）日本語の文を英語に翻訳するプログラムを実装せよ．"
      ],
      "metadata": {
        "id": "jH7MOwWpyR4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 92 機械翻訳モデルの適用\n",
        "\n",
        "!fairseq-interactive --path /content/checkpoints/checkpoint_best.pt /content/data < /content/data/test.ja | grep '^H' | cut -f3 > translate_92.out"
      ],
      "metadata": {
        "id": "qec6hAykyR_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "93.91で学習したニューラル機械翻訳モデルの品質を調べるため，評価データにおけるBLEUスコアを測定せよ．"
      ],
      "metadata": {
        "id": "2NicXt0XySFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 93 BLUEスコアの計測\n",
        "\n",
        "!fairseq-score --sys translate_92.out --ref /content/data/test.en"
      ],
      "metadata": {
        "id": "vOuWYZd_ySLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "94.91で学習したニューラル機械翻訳モデルで翻訳文をデコードする際に，ビーム探索を導入せよ．ビーム幅を1から100くらいまで適当に変化させながら，開発セット上のBLEUスコアの変化をプロットせよ．"
      ],
      "metadata": {
        "id": "LcI0WVzZySRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 94 ビーム探索\n",
        "\n",
        "!fairseq-interactive /content/data --path \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best.pt\" < /content/data/test.ja --beam 1 | grep '^H' | cut -f3 > translate_92_1.out\n",
        "!fairseq-interactive /content/data --path \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best.pt\" < /content/data/test.ja --beam 25 | grep '^H' | cut -f3 > translate_92_25.out\n",
        "!fairseq-interactive /content/data --path \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best.pt\" < /content/data/test.ja --beam 50 | grep '^H' | cut -f3 > translate_92_50.out\n",
        "!fairseq-interactive /content/data --path \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best.pt\" < /content/data/test.ja --beam 75 | grep '^H' | cut -f3 > translate_92_75.out\n",
        "!fairseq-interactive /content/data --path \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best.pt\" < /content/data/test.ja --beam 100 | grep '^H' | cut -f3 > translate_92_100.out"
      ],
      "metadata": {
        "id": "Qc9s6CjnySXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-score --sys translate_92_1.out --ref /content/data/test.en > /content/BLEU_score_1\n",
        "!fairseq-score --sys translate_92_25.out --ref /content/data/test.en > /content/BLEU_score_25\n",
        "!fairseq-score --sys translate_92_50.out --ref /content/data/test.en > /content/BLEU_score_50\n",
        "!fairseq-score --sys translate_92_75.out --ref /content/data/test.en > /content/BLEU_score_75\n",
        "!fairseq-score --sys translate_92_100.out --ref /content/data/test.en > /content/BLEU_score_100"
      ],
      "metadata": {
        "id": "2WFkoZ4p4hcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "def cal_score(file):\n",
        "    with open(file) as f:\n",
        "        line = f.readlines()[1]\n",
        "        line = re.search(r\"(?<=BLEU4 = )\\d*\\.\\d*(?=,)\", line)\n",
        "        return float(line.group())\n",
        "\n",
        "x = [1, 25, 50, 75, 100]\n",
        "y = [cal_score(\"/content/BLEU_score_{}\".format(n)) for n in x]\n",
        "plt.plot(x, y)\n",
        "plt.xlabel(\"beam_size\")\n",
        "plt.ylabel(\"BLEU\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cpm8GaqD4pae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "95.トークンの単位を単語や形態素からサブワードに変更し，91-94の実験を再度実施せよ．"
      ],
      "metadata": {
        "id": "dq6N55uwySdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install subword-nmt"
      ],
      "metadata": {
        "id": "rKdKI-KV5Doi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 95 サブワード化\n",
        "\n",
        "import sentencepiece as spm\n",
        "import re\n",
        "\n",
        "spm.SentencePieceTrainer.Train(\"--input=kftt-data-1.0/data/orig/kyoto-train.ja --model_prefix=kyoto_ja --vocab_size=16000 --character_coverage=1.0\")\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"kyoto_ja.model\")\n",
        "\n",
        "for src, dst in [\n",
        "    (\"kftt-data-1.0/data/orig/kyoto-train.ja\", \"/content/data/train.ja\"),\n",
        "    (\"kftt-data-1.0/data/orig/kyoto-dev.ja\", \"/content/data/dev.ja\"),\n",
        "    (\"kftt-data-1.0/data/orig/kyoto-test.ja\", \"/content/data/test.ja\"),\n",
        "]:\n",
        "    with open(src) as f, open(dst, 'w') as g:\n",
        "        for x in f:\n",
        "            x = x.strip()\n",
        "            x = re.sub(r'\\s+', ' ', x)\n",
        "            x = sp.encode_as_pieces(x)\n",
        "            x = \" \".join(x)\n",
        "            print(x, file=g)"
      ],
      "metadata": {
        "id": "pVj_Gw_-ySjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!subword-nmt learn-bpe -s 16000 < kftt-data-1.0/data/orig/kyoto-train.en > kyoto_en.codes\n",
        "!subword-nmt apply-bpe -c kyoto_en.codes < kftt-data-1.0/data/orig/kyoto-train.en > /content/data/train.en\n",
        "!subword-nmt apply-bpe -c kyoto_en.codes < kftt-data-1.0/data/orig/kyoto-dev.en > /content/data/dev.en\n",
        "!subword-nmt apply-bpe -c kyoto_en.codes < kftt-data-1.0/data/orig/kyoto-test.en > /content/data/test.en"
      ],
      "metadata": {
        "id": "TlOVcBtT59im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 前処理\n",
        "\n",
        "!git clone https://github.com/pytorch/fairseq\n",
        "%cd /content/fairseq/\n",
        "!python -m pip install --editable .\n",
        "%cd /content\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "\n",
        "!fairseq-preprocess -s ja -t en \\\n",
        "    --trainpref /content/data/train \\\n",
        "    --validpref /content/data/dev \\\n",
        "    --destdir /content/data \\\n",
        "    --thresholdsrc 5 \\\n",
        "    --thresholdtgt 5 \\\n",
        "    --workers 20"
      ],
      "metadata": {
        "id": "7EnlT1Rk6YUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練\n",
        "\n",
        "!fairseq-train /content/data --arch transformer \\\n",
        "    --criterion label_smoothed_cross_entropy \\\n",
        "    --label-smoothing 0.1 \\\n",
        "    --lr 1e-5 \\\n",
        "    --lr-scheduler inverse_sqrt \\\n",
        "    --warmup-updates 3000 \\\n",
        "    --optimizer adam \\\n",
        "    --max-tokens 4000 \\\n",
        "    --max-epoch 3 \\\n",
        "    --dropout 0.1 \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --weight-decay 0.0001 \\\n",
        "    --patience 5 \\\n",
        "    --no-epoch-checkpoints > 95.log \\"
      ],
      "metadata": {
        "id": "yAbwNUcK6t_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! cp /content/checkpoints/checkpoint_best.pt \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best_2.pt\""
      ],
      "metadata": {
        "id": "iZ5Py_zN6y68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの適用\n",
        "!fairseq-interactive --path \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best_2.pt\" /content/data < /content/data/test.ja | grep '^H' | cut -f3 > translate_95.out"
      ],
      "metadata": {
        "id": "ykxoRjbu7JtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLUEスコアの計測\n",
        "!fairseq-score --sys translate_95.out --ref /content/data/test.en"
      ],
      "metadata": {
        "id": "QuV5-8CT7OZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ビーム探索\n",
        "!fairseq-interactive /content/data --path \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best_2.pt\" < /content/data/test.ja --beam 1 | grep '^H' | cut -f3 > translate_95_1.out\n",
        "!fairseq-interactive /content/data --path \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best_2.pt\" < /content/data/test.ja --beam 25 | grep '^H' | cut -f3 > translate_95_25.out\n",
        "!fairseq-interactive /content/data --path \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best_2.pt\" < /content/data/test.ja --beam 50 | grep '^H' | cut -f3 > translate_95_50.out\n",
        "!fairseq-interactive /content/data --path \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best_2.pt\" < /content/data/test.ja --beam 75 | grep '^H' | cut -f3 > translate_95_75.out\n",
        "!fairseq-interactive /content/data --path \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best_2.pt\" < /content/data/test.ja --beam 100 | grep '^H' | cut -f3 > translate_95_100.out\n",
        "\n",
        "!fairseq-score --sys translate_95_1.out --ref /content/data/test.en > /content/BLEU_score_1\n",
        "!fairseq-score --sys translate_95_25.out --ref /content/data/test.en > /content/BLEU_score_25\n",
        "!fairseq-score --sys translate_95_50.out --ref /content/data/test.en > /content/BLEU_score_50\n",
        "!fairseq-score --sys translate_95_75.out --ref /content/data/test.en > /content/BLEU_score_75\n",
        "!fairseq-score --sys translate_95_100.out --ref /content/data/test.en > /content/BLEU_score_100"
      ],
      "metadata": {
        "id": "uaJxwn0g9Bd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "def cal_score(file):\n",
        "    with open(file) as f:\n",
        "        line = f.readlines()[1]\n",
        "        line = re.search(r\"(?<=BLEU4 = )\\d*\\.\\d*(?=,)\", line)\n",
        "        return float(line.group())\n",
        "\n",
        "x = [1, 25, 50, 75, 100]\n",
        "y = [cal_score(\"/content/BLEU_score_{}\".format(n)) for n in x]\n",
        "plt.plot(x, y)\n",
        "plt.xlabel(\"beam_size\")\n",
        "plt.ylabel(\"BLEU\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hnAYgV5K-I38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "96.Tensorboardなどのツールを用い，ニューラル機械翻訳モデルが学習されていく過程を可視化せよ．可視化する項目としては，学習データにおける損失関数の値とBLEUスコア，開発データにおける損失関数の値とBLEUスコアなどを採用せよ．"
      ],
      "metadata": {
        "id": "yMBFZtNeyhFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "A5glYJWcyhLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 96 学習過程の可視化\n",
        "\n",
        "!fairseq-train /content/data --arch transformer\\\n",
        "    --tensorboard-logdir log96 \\\n",
        "    --save-dir save96 \\\n",
        "    --max-epoch 3 \\\n",
        "    --optimizer adam --clip-norm 1.0 \\\n",
        "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 3000 \\\n",
        "    --dropout 0.1 --weight-decay 0.0001 \\\n",
        "    --update-freq 1 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-tokens 4000 > 96.log"
      ],
      "metadata": {
        "id": "fxRALdRR-ORB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir log96"
      ],
      "metadata": {
        "id": "OoYGQlUV-_wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "97.ニューラルネットワークのモデルや，そのハイパーパラメータを変更しつつ，開発データにおけるBLEUスコアが最大となるモデルとハイパーパラメータを求めよ."
      ],
      "metadata": {
        "id": "GBdyRlrkyhRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 97 ハイパー・パラメータの調整\n",
        "\n",
        "!fairseq-train /content/data --arch transformer \\\n",
        "    --save-dir save97 \\\n",
        "    --max-epoch 3 \\\n",
        "    --optimizer adam --clip-norm 1.0 \\\n",
        "    --lr 3e-3 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --weight-decay 0.0001 \\\n",
        "    --update-freq 1 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-tokens 6000 > 97.log\n",
        "\n",
        "!cp /content/save97/checkpoint_best.pt \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best_3.pt\"\n",
        "\n",
        "!fairseq-interactive --path \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best_3.pt\" /content/data < /content/data/test.ja --beam 10 | grep '^H' | cut -f3 > translate_97.out\n",
        "\n",
        "!fairseq-score --sys translate_97.out --ref /content/data/test.en"
      ],
      "metadata": {
        "id": "q4qeJO6OyhXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "98.Japanese-English Subtitle Corpus (JESC)やJParaCrawlなどの翻訳データを活用し，KFTTのテストデータの性能向上を試みよ．"
      ],
      "metadata": {
        "id": "Emhng_tWyhdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 98 ドメイン適応\n",
        "\n",
        "!fairseq-preprocess -s ja -t en \\\n",
        "    --trainpref \"/content/drive/MyDrive/Colab Notebooks/train.jesc\" \\\n",
        "    --validpref /content/data/dev \\\n",
        "    --destdir /content/data \\\n",
        "    --thresholdsrc 5 \\\n",
        "    --thresholdtgt 5 \\\n",
        "    --workers 20\n",
        "\n",
        "!fairseq-train /content/data --arch transformer \\\n",
        "    --criterion label_smoothed_cross_entropy \\\n",
        "    --label-smoothing 0.1 \\\n",
        "    --lr 1e-5 \\\n",
        "    --lr-scheduler inverse_sqrt \\\n",
        "    --warmup-updates 3000 \\\n",
        "    --optimizer adam \\\n",
        "    --max-tokens 4000 \\\n",
        "    --max-epoch 1 \\\n",
        "    --dropout 0.1 \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --weight-decay 0.0001 \\\n",
        "    --patience 5 \\\n",
        "    --no-epoch-checkpoints > 98_1.log \\\n",
        "\n",
        "! cp /content/checkpoints/checkpoint_best.pt \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best_5.pt\"\n",
        "\n",
        "!fairseq-interactive --path \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best_5.pt\" /content/data < /content/data/test.ja | grep '^H' | cut -f3 > translate_98_2.out\n",
        "\n",
        "!fairseq-score --sys translate_98_2.out --ref /content/data/test.en"
      ],
      "metadata": {
        "id": "n-RpOmsPyhiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "99.ユーザが翻訳したい文を入力すると，その翻訳結果がウェブブラウザ上で表示されるデモシステムを構築せよ."
      ],
      "metadata": {
        "id": "p1jWOLDpyhoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install fairseq"
      ],
      "metadata": {
        "id": "unxVGc1Lyhu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, render_template, request\n",
        "from fairseq.models.transformer import TransformerModel\n",
        "\n",
        "model = TransformerModel.from_pretrained(\n",
        "    '/content/checkpoints/',\n",
        "    checkpoint_file=\"checkpoint_best.pt\",\n",
        "    data_name_or_path=\"/content/data\",\n",
        ")\n",
        "\n",
        "app = Flask(__name__,template_folder=\"(前略)/templates\")\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\"])\n",
        "def get():\n",
        "    return render_template(\"index.html\", \\\n",
        "        title = \"翻訳\", \\\n",
        "        message = \"入力してください\")\n",
        "\n",
        "@app.route(\"/\", methods=[\"POST\"])\n",
        "def post():\n",
        "    text = request.form[\"name\"]\n",
        "    return render_template(\"index.html\", \\\n",
        "        title = \"翻訳\", \\\n",
        "        output = model.translate(text, beam=1)\n",
        "        message = text+\"　　　　　　　→　\"+output)\n",
        "\n",
        "app.run()"
      ],
      "metadata": {
        "id": "oatYjBNxAxhk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}